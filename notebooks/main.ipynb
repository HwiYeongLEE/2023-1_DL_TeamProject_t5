{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    DonutProcessor,\n",
    "    VisionEncoderDecoderConfig,\n",
    "    VisionEncoderDecoderModel,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device.type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'WORKING_DIR': \"/home/2023-1_DL_TeamProject_t5\",\n",
    "    'SEED':42,\n",
    "    'NUM_WORKERS':4,\n",
    "    'IMG_HEIGHT':4032,\n",
    "    'IMG_WIDTH':3024,\n",
    "    'MAX_LEN':1024,\n",
    "    'BATCH_SIZE':2\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Working Direcotry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/2023-1_DL_TeamProject_t5\n"
     ]
    }
   ],
   "source": [
    "os.chdir(CFG['WORKING_DIR'])\n",
    "print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix Seeds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Building & Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {0:\"uni\", 1:\"nm\", 2:\"ing\", 3:\"exp\", 4:\"how\", 5:\"des\", 9:\"etc\"}\n",
    "\n",
    "class DonutDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        max_length: int,\n",
    "        processor: DonutProcessor,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.dataframe = dataframe\n",
    "        self.dataframe_length = len(self.dataframe)\n",
    "        self.processor = processor\n",
    "        self.gt_container = []\n",
    "        \n",
    "        for idx, sample in self.dataframe.iterrows():\n",
    "            ground_truth = self.get_gt_strings(eval(sample['texts']))\n",
    "            self.gt_container.append(ground_truth)\n",
    "\n",
    "    def get_gt_strings(self, ct):\n",
    "        \n",
    "        gt_string = \"\"\n",
    "        flag = 1\n",
    "        tp = -1\n",
    "        for i, item in enumerate(ct):\n",
    "            if flag:\n",
    "                gt_string = gt_string + f'<{type_dict[item[0]]}>'\n",
    "                tp = item[0]\n",
    "                flag = 0\n",
    "                gt_string = gt_string + f'{item[1]}'\n",
    "            \n",
    "            elif not flag:\n",
    "                gt_string = gt_string + f' {item[1]}'\n",
    "            \n",
    "            if i == len(ct)-1 or ct[i+1][0] != tp:\n",
    "                gt_string = gt_string + f'</{type_dict[item[0]]}>'\n",
    "                flag = 1\n",
    "        \n",
    "        return gt_string\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.dataframe_length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        sample = self.dataframe.loc[idx]\n",
    "        image = Image.open(sample['image_path'])\n",
    "       \n",
    "        pixel_values = self.processor(image, random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        target_sequence = self.gt_container[idx] \n",
    "        input_ids = self.processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = self.ignore_id  \n",
    "\n",
    "        return pixel_values, labels, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "processor.image_processor.size = {\"height\": CFG['IMG_HEIGHT'],\"width\": CFG['IMG_WIDTH']}\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "config.encoder.image_size = [CFG['IMG_HEIGHT'], CFG['IMG_WIDTH']]\n",
    "config.decoder.max_length = CFG['MAX_LEN']\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = [fr'<{x}>' for x in type_dict.values()] + [fr'</{x}>' for x in type_dict.values()]\n",
    "processor.tokenizer.add_tokens(added_tokens)\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df = pd.read_csv(\"./dataframes/train_annot_df.csv\")\n",
    "test_df = pd.read_csv(\"./dataframes/test_annot_df.csv\")\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=CFG['SEED'])\n",
    "\n",
    "train_dataset = DonutDataset(train_df, max_length=CFG['MAX_LEN'], processor=processor, split=\"train\")\n",
    "val_dataset = DonutDataset(val_df, max_length=CFG['MAX_LEN'], processor=processor, split=\"validation\")\n",
    "test_dataset = DonutDataset(test_df, max_length=CFG['MAX_LEN'], processor=processor, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "pixel_values, labels, target_sequence = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4032, 3024])\n"
     ]
    }
   ],
   "source": [
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<how>\n",
      "[\n",
      "사\n",
      "용\n",
      "시\n",
      "의\n",
      "\n",
      "주의\n",
      "사항\n",
      "]\n",
      "1)\n",
      "화\n",
      "장\n",
      "품\n",
      "사용\n",
      "시\n",
      "또는\n",
      "사용\n",
      "후\n",
      "직\n",
      "사\n",
      "광\n",
      "선\n",
      "에\n",
      "의\n",
      "하여\n",
      "사용\n",
      "부\n",
      "위\n",
      "가\n"
     ]
    }
   ],
   "source": [
    "for id in labels.tolist()[:30]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<how>[사용시의 주의사항] 1) 화장품 사용 시 또는 사용 후 직사광선에 의하여 사용부위가 붉은반점, 부어오름 또 는 가려움증 등의 이상 증상이나 부작용이 있는 경우 전문의 등과 상담할 것 2) 상처가 있는 부위 등에는 사용을 자제할 것 3) 보관 및 취급시의 주의사항 가) 어린이의 손이 닿지 않는 곳에 보관할 것 나) 직사광선을 피해서 보관할 것</how><des>품번 : 1020691</des><nm>품명 : 리더스더마소울메닛 오드퍼퓸_블루</nm><how>[사용법] 맥박이 뛰는 곳에 이를 분사하여 향기가 나게 합니다. 스프레 사용</how><des>화장품책임판매업자 : 스메틱 (주) 리더스코 경기도 안성시 미양면 제4 산단1로 34 화장품제조업자 : (주) 킨팜 경기도 김포시 월곶면 애기봉 로 456번길 72 제조번호 및</des><exp>사용기한 : 별도표기</exp><des>용량 : 30ml 소비자상담실 : 080-866-6868 · 본 제품은 공정거래위원회 고시 소비자 분쟁 해결기준에 의거 교환 받을 수 있습니다. 또는 보상 MADE IN KOREA YM DB ZM 8 808739 000306</des>\n"
     ]
    }
   ],
   "source": [
    "print(target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token ID: <pad>\n",
      "Decoder start token ID: <s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
    "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloader Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

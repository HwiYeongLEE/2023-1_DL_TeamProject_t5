{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from transformers import (\n",
    "    DonutProcessor,\n",
    "    VisionEncoderDecoderConfig,\n",
    "    VisionEncoderDecoderModel,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device.type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'SEED':42,\n",
    "    'NUM_WORKERS':4,\n",
    "    'NUM_PROC':1,\n",
    "    'IMG_HEIGHT':0,\n",
    "    'IMG_WIDTH':0,\n",
    "    'MAX_LEN':10000\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix Seeds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {0:\"unk\", 1:\"nm\", 2:\"ing\", 3:\"exp\", 4:\"how\", 5:\"des\", 9:\"etc\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_strings(ct):\n",
    "    gt_string = \"\"\n",
    "    flag = 1\n",
    "    tp = -1\n",
    "    for i, item in enumerate(ct):\n",
    "        if flag:\n",
    "            gt_string = gt_string + f'<{type_dict[item[0]]}>'\n",
    "            tp = item[0]\n",
    "            flag = 0\n",
    "            gt_string = gt_string + f'{item[1]}'\n",
    "        \n",
    "        elif not flag:\n",
    "            gt_string = gt_string + f' {item[1]}'\n",
    "        \n",
    "        if i == len(ct)-1 or ct[i+1][0] != tp:\n",
    "            gt_string = gt_string + f'</{type_dict[item[0]]}>'\n",
    "            flag = 1\n",
    "    \n",
    "    return gt_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------Save Above------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(df, split='train'):\n",
    "    \n",
    "    img_path = Path(\"../data/train\") if split == 'train' else Path(\"../data/test\")\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "         yield {\n",
    "              \"id\": row['image_path'].split('.')[0],\n",
    "              \"groud_truth\": get_gt_strings(eval(row['texts'])),\n",
    "              \"image_path\": img_path / row['image_path'], \n",
    "              \"image_height\": row['image_size'][0],\n",
    "              \"image_width\": row['image_size'][1],\n",
    "              \"image_class\": row['image_class'],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../dataframes/train_annot_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = get_gt_strings(eval(train_df.loc[0, 'texts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = processor.tokenizer(temp).input_ids\n",
    "tokens = [processor.tokenizer.tokenize(x, add_special_tokens=True) for x in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m example_ids, example_tokens \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ids, tokens):\n\u001b[1;32m      3\u001b[0m     example_unk_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(example_ids)):\n\u001b[1;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m example_ids[i] \u001b[39m==\u001b[39m processor\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39munk_token_id:\n\u001b[1;32m      6\u001b[0m             example_unk_tokens\u001b[39m.\u001b[39mappend(example_tokens[i])\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "unk_tokens = []\n",
    "for example_ids, example_tokens in zip(ids, tokens):\n",
    "    example_unk_tokens = []\n",
    "    for i in range(len(example_ids)):\n",
    "        if example_ids[i] == processor.tokenizer.unk_token_id:\n",
    "            example_unk_tokens.append(example_tokens[i])\n",
    "\n",
    "    unk_tokens.append(example_unk_tokens)\n",
    "\n",
    "unk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "\n",
    "for id in ids:\n",
    "    decoded.append(processor.decode([id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<',\n",
       " 'n',\n",
       " 'm',\n",
       " '>',\n",
       " 'Do',\n",
       " 've',\n",
       " 'she',\n",
       " 'a',\n",
       " 'but',\n",
       " 'ter',\n",
       " '시',\n",
       " '어',\n",
       " '버',\n",
       " '터',\n",
       " '',\n",
       " '<unk>',\n",
       " '티',\n",
       " '바',\n",
       " 'beauty',\n",
       " 'cream',\n",
       " 'bar',\n",
       " '</',\n",
       " 'n',\n",
       " 'm',\n",
       " '><',\n",
       " 'des',\n",
       " '>',\n",
       " 'B',\n",
       " 'eau',\n",
       " 'ty',\n",
       " 'bar',\n",
       " 'with',\n",
       " 'This',\n",
       " 'beauty',\n",
       " 'bar',\n",
       " 'la',\n",
       " 'ther',\n",
       " 'that',\n",
       " 'will',\n",
       " 'with',\n",
       " 'leave',\n",
       " 'your',\n",
       " 'skin',\n",
       " 'feeling',\n",
       " 'she',\n",
       " 'a',\n",
       " 'but',\n",
       " 'ter',\n",
       " 'and',\n",
       " 'warm',\n",
       " '1',\n",
       " '<unk>',\n",
       " '4',\n",
       " 'mois',\n",
       " 'tur',\n",
       " 'izing',\n",
       " 'van',\n",
       " 'illa',\n",
       " 'scen',\n",
       " 't',\n",
       " '.',\n",
       " 'cream',\n",
       " 'create',\n",
       " 's',\n",
       " 'a',\n",
       " 'cream',\n",
       " 'y',\n",
       " 'resto',\n",
       " 'red',\n",
       " '.',\n",
       " '시',\n",
       " '어',\n",
       " '버',\n",
       " '터',\n",
       " '와',\n",
       " '1',\n",
       " '<unk>',\n",
       " '4',\n",
       " '부',\n",
       " '드',\n",
       " '<unk>',\n",
       " '게',\n",
       " '가',\n",
       " '꿔',\n",
       " '줍니다',\n",
       " '.',\n",
       " '모',\n",
       " '이',\n",
       " '스',\n",
       " '처',\n",
       " '라이',\n",
       " '징',\n",
       " '달',\n",
       " '<unk>',\n",
       " '한',\n",
       " '바',\n",
       " '<unk>',\n",
       " '라',\n",
       " '향',\n",
       " '의',\n",
       " '',\n",
       " '<unk>',\n",
       " '티',\n",
       " '바',\n",
       " '크',\n",
       " '림',\n",
       " '을',\n",
       " '함',\n",
       " '유',\n",
       " '한',\n",
       " '',\n",
       " '<unk>',\n",
       " '티',\n",
       " '바',\n",
       " '의',\n",
       " '크',\n",
       " '리',\n",
       " '미',\n",
       " '한',\n",
       " '거',\n",
       " '품',\n",
       " '이',\n",
       " '피',\n",
       " '부를',\n",
       " '',\n",
       " '촉',\n",
       " '촉',\n",
       " '하고',\n",
       " '</',\n",
       " 'des',\n",
       " '><',\n",
       " 'exp',\n",
       " '>',\n",
       " '20',\n",
       " '24.',\n",
       " '04.',\n",
       " '01',\n",
       " '',\n",
       " '까지',\n",
       " '</',\n",
       " 'exp',\n",
       " '>',\n",
       " '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = []\n",
    "\n",
    "class DonutDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe_path: str,\n",
    "        max_length: int,\n",
    "        processor: DonutProcessor,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.dataframe = pd.read_csv(\"../dataframes/train_annot_df.csv\") if self.split == \"train\" else  pd.read_csv(\"../dataframes/test_annot_df.csv\")\n",
    "        self.dataframe_length = len(self.dataframe)\n",
    "        self.gt_token_sequences = []\n",
    "        \n",
    "        for idx, sample in self.dataframe.iterrows():\n",
    "            ground_truth = get_gt_strings(eval(sample['texts']))\n",
    "            if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                gt_jsons = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        gt_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    + processor.tokenizer.eos_token\n",
    "                    for gt_json in gt_jsons  # load json from list of json\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"\", fr\"\"])\n",
    "                    output += (\n",
    "                        fr\"\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "    \n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
    "        Convert gt data into input_ids (tokenized string)\n",
    "        Returns:\n",
    "            input_tensor : preprocessed image\n",
    "            input_ids : tokenized gt_data\n",
    "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        pixel_values = processor(sample[\"image\"], random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.squeeze()\n",
    "\n",
    "        # targets\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n",
    "        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
    "        return pixel_values, labels, target_sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
